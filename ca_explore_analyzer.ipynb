{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EEU0_eDm1VWW"
      },
      "id": "EEU0_eDm1VWW"
    },
    {
      "cell_type": "code",
      "id": "fRDHBhQ9UoFm7hOKauI5RSDv",
      "metadata": {
        "tags": [],
        "id": "fRDHBhQ9UoFm7hOKauI5RSDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd436bce-1e8f-4956-ad12-3eaea0abfcd3"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# This script requires the Looker SDK, Google Cloud Secret Manager,\n",
        "# and the Vertex AI SDK to be installed.\n",
        "# Please install them using pip if you haven't already:\n",
        "# !pip install looker-sdk google-cloud-secret-manager google-cloud-aiplatform --upgrade --user\n",
        "\n",
        "from google.cloud import secretmanager\n",
        "import os\n",
        "import looker_sdk\n",
        "# Removed problematic WriteQuery/models imports. Will construct query body as dict.\n",
        "import configparser\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "import sys # For potential error handling exits\n",
        "import json # For potentially saving results\n",
        "import time # For potential delays between API calls\n",
        "import collections # For defaultdict\n",
        "import typing # For type hinting if needed\n",
        "import textwrap # For formatting comments\n",
        "import re # For parsing recommendations\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Google Cloud Project and Secret Manager details for Looker credentials\n",
        "# !!! REPLACE WITH YOUR ACTUAL VALUES !!!\n",
        "looker_project_id = \"joey-looker\" # Project where the secret is stored\n",
        "looker_secret_name = \"looker_ini\"     # Name of the secret in Secret Manager\n",
        "looker_secret_version = \"latest\"                   # Use 'latest' or a specific version number\n",
        "\n",
        "# Vertex AI Configuration\n",
        "# !!! REPLACE WITH YOUR ACTUAL VALUES !!!\n",
        "vertex_project_id = \"joey-looker\" # Project where Vertex AI is enabled\n",
        "vertex_location = \"us-central1\"                   # Choose a location supporting the model (e.g., \"us-central1\")\n",
        "# Choose the Gemini model (check Vertex AI documentation for available models)\n",
        "gemini_model_name = \"gemini-2.5-pro-exp-03-25\"        # Example: Use a recent, capable model like 1.5 Flash or Pro\n",
        "\n",
        "# Optional: Delay between Gemini API calls in seconds (to help avoid rate limits)\n",
        "API_CALL_DELAY = 1 # Set to 0 for no delay\n",
        "\n",
        "# --- History Data Configuration ---\n",
        "# History data is now fetched via API using parameters below\n",
        "TOP_N_FIELDS = 15 # Number of top fields to report per explore based on usage\n",
        "AGENT_INSTRUCTION_TOP_FIELDS = 5 # Number of top fields to include in agent instructions\n",
        "\n",
        "# Define weights for different query sources\n",
        "# Higher weight for interactive sources, lower for passive/automated\n",
        "SOURCE_WEIGHTS = {\n",
        "    # Higher weight for interactive sources\n",
        "    \"explore\": 3.0,\n",
        "    \"drill_modal\": 3.0, # Renamed from drill_down\n",
        "    \"suggest\": 2.0,    # Suggestions imply interaction\n",
        "    \"merge_query\": 2.0,# Merged queries are actively created\n",
        "    \"api\": 1.5,        # API calls can be varied, moderate weight\n",
        "    \"sql_runner\": 3.0, # Direct user interaction\n",
        "\n",
        "    # Lower weight for automated/passive sources\n",
        "    \"dashboard\": 1.0,\n",
        "    \"look\": 1.0,\n",
        "    \"scheduled_task\": 0.5, # Low weight for scheduled tasks\n",
        "    \"cache\": 0.0,      # Ignore cache hits entirely\n",
        "    \"embed\": 1.0,      # Embed usage might be similar to dashboard\n",
        "    # Add other potential sources if needed\n",
        "}\n",
        "DEFAULT_WEIGHT = 0.5 # Default weight for sources not explicitly listed\n",
        "\n",
        "# --- Target Explores Configuration ---\n",
        "# Define specific explores to analyze (format: [\"model_name/explore_name\", ...])\n",
        "# If empty (TARGET_EXPLORES = []), the script will analyze ALL non-hidden explores.\n",
        "# Example: TARGET_EXPLORES = [\"my_model/orders\", \"my_other_model/users\"]\n",
        "TARGET_EXPLORES = [\"basic_ecomm/basic_order_items\"]\n",
        "\n",
        "# --- CA LookML Naming Convention ---\n",
        "CA_SUFFIX = \"_ca\" # Suffix to add to extended views and explores for CA\n",
        "\n",
        "\n",
        "# --- Secret Manager Setup ---\n",
        "def get_looker_config_from_secret_manager(project_id, secret_name, version):\n",
        "    \"\"\"Fetches Looker config from Secret Manager.\"\"\"\n",
        "    print(f\"Attempting to fetch secret '{secret_name}' version '{version}' from project '{project_id}'...\")\n",
        "    try:\n",
        "        client = secretmanager.SecretManagerServiceClient()\n",
        "        secret_id = f\"projects/{project_id}/secrets/{secret_name}/versions/{version}\"\n",
        "        response = client.access_secret_version(request={\"name\": secret_id})\n",
        "        secret_string = response.payload.data.decode(\"UTF-8\")\n",
        "        print(f\"Successfully fetched secret '{secret_name}'.\")\n",
        "\n",
        "        config = configparser.ConfigParser()\n",
        "        config.read_string(secret_string)\n",
        "\n",
        "        # Validate required keys exist in the config\n",
        "        if 'Looker' not in config or not all(k in config['Looker'] for k in ['base_url', 'client_id', 'client_secret']):\n",
        "             missing_keys = {'base_url', 'client_id', 'client_secret'} - set(config.get('Looker', {}).keys())\n",
        "             raise ValueError(f\"Missing required keys {missing_keys} in [Looker] section of the secret '{secret_name}'.\")\n",
        "\n",
        "        print(\"Successfully parsed [Looker] section from secret.\")\n",
        "        return config['Looker']\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to fetch or parse secrets from Secret Manager: {e}\")\n",
        "        print(\"Troubleshooting steps:\")\n",
        "        print(f\"  1. Verify the secret '{secret_name}' exists in project '{project_id}'.\")\n",
        "        print(\"  2. Ensure the secret is formatted as INI with a [Looker] section containing 'base_url', 'client_id', 'client_secret'.\")\n",
        "        print(\"  3. Check that the service account/user running this script has the 'Secret Manager Secret Accessor' IAM role.\")\n",
        "        print(f\"  4. Confirm '{project_id}' is the correct Google Cloud project ID.\")\n",
        "        sys.exit(1) # Exit if we can't get credentials\n",
        "\n",
        "# --- Looker SDK Setup ---\n",
        "def initialize_looker_sdk(config):\n",
        "    \"\"\"Initializes the Looker SDK using config.\"\"\"\n",
        "    print(\"Initializing Looker SDK...\")\n",
        "    try:\n",
        "        # Set environment variables for the SDK from the fetched config\n",
        "        os.environ[\"LOOKERSDK_BASE_URL\"] = config['base_url']\n",
        "        os.environ[\"LOOKERSDK_CLIENT_ID\"] = config['client_id']\n",
        "        os.environ[\"LOOKERSDK_CLIENT_SECRET\"] = config['client_secret']\n",
        "        # Optional: Add environment variables for SSL verification or timeout if needed\n",
        "        # Default to verifying SSL unless explicitly set to 'false' in the secret\n",
        "        os.environ[\"LOOKERSDK_VERIFY_SSL\"] = config.get('verify_ssl', \"true\").lower()\n",
        "        # Default to a 120-second timeout if not specified in the secret\n",
        "        os.environ[\"LOOKERSDK_TIMEOUT\"] = config.get('timeout', \"120\")\n",
        "\n",
        "        # Use init40() for Looker API version 4.0\n",
        "        sdk = looker_sdk.init40()\n",
        "\n",
        "        # Test connection by fetching the current user (requires 'see_users' permission)\n",
        "        user = sdk.me(fields=\"id,display_name\") # Request specific fields\n",
        "        print(f\"Looker SDK initialized successfully. Connected as user: {user.display_name} (ID: {user.id})\")\n",
        "        return sdk\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to initialize Looker SDK or verify connection: {e}\")\n",
        "        print(\"Troubleshooting steps:\")\n",
        "        print(\"  1. Verify 'base_url', 'client_id', and 'client_secret' in your secret are correct.\")\n",
        "        print(\"  2. Ensure the Looker API user associated with the credentials exists and is enabled.\")\n",
        "        print(\"  3. Check that the Looker API user has sufficient permissions (e.g., 'see_lookml_dashboards', 'see_user_dashboards', 'explore', 'access_data', 'see_looks', 'see_users').\")\n",
        "        print(f\"  4. Confirm the Looker instance is reachable at: {config.get('base_url', 'URL NOT FOUND IN CONFIG')}\")\n",
        "        print(f\"  5. Check LOOKERSDK_VERIFY_SSL setting (currently: {os.environ.get('LOOKERSDK_VERIFY_SSL', 'Not Set')}). Set to 'false' in secret if using self-signed certs (use with caution).\")\n",
        "        sys.exit(1) # Exit if SDK initialization fails\n",
        "\n",
        "\n",
        "# --- Vertex AI Setup ---\n",
        "def initialize_vertex_ai(project_id, location):\n",
        "    \"\"\"Initializes the Vertex AI environment.\"\"\"\n",
        "    print(f\"Initializing Vertex AI for project '{project_id}' in location '{location}'...\")\n",
        "    try:\n",
        "        vertexai.init(project=project_id, location=location)\n",
        "        print(f\"Vertex AI initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to initialize Vertex AI: {e}\")\n",
        "        print(\"Troubleshooting steps:\")\n",
        "        print(f\"  1. Ensure the Vertex AI API is enabled for project '{project_id}'.\")\n",
        "        print(\"  2. Verify your environment is authenticated to Google Cloud (e.g., run 'gcloud auth application-default login' locally, or check service account permissions on Cloud resources).\")\n",
        "        print(\"  3. Confirm the authenticated user/service account has necessary Vertex AI permissions (e.g., 'Vertex AI User' role).\")\n",
        "        print(f\"  4. Check if the location '{location}' is valid and supports the intended Gemini model.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# --- History Data Processing ---\n",
        "# Removed problematic type hint looker_sdk.SDKClient for 'sdk' parameter\n",
        "def fetch_and_process_history(sdk, weights: dict, default_weight: float):\n",
        "    \"\"\"Fetches Looker history data via API and processes it to calculate weighted field usage.\"\"\"\n",
        "    print(\"Fetching and processing history data via Looker API...\")\n",
        "\n",
        "    # Define the parameters for the inline query based on the provided URL\n",
        "    model_name = \"system__activity\"\n",
        "    explore_name = \"history\" # Use explore name as 'view' for run_inline_query\n",
        "    fields = [\n",
        "        \"query.view\",             # Explore name used in the query\n",
        "        \"history.query_run_count\",# Number of times the query ran\n",
        "        \"query.model\",            # Model name used\n",
        "        \"query.fields\",           # List of fields used (view_name.field_name)\n",
        "        \"history.source\",         # Source of the query (dashboard, explore, etc.)\n",
        "        \"user.count\"              # *** ADDED user.count field ***\n",
        "    ]\n",
        "    filters = {\n",
        "        \"history.created_date\": \"90 days\",       # Filter for the last 90 days\n",
        "        \"query.model\": \"-NULL,-system__activity\" # Exclude NULL models and system__activity itself\n",
        "        # Assuming empty matches_filter in URL means no filter on source and view\n",
        "    }\n",
        "    sorts = [\"history.query_run_count desc\"] # Sort by run count descending\n",
        "    limit = \"5000\" # API expects limit as string, match URL\n",
        "\n",
        "    # Construct the query body as a dictionary instead of WriteQuery object\n",
        "    query_body_dict = {\n",
        "        \"model\": model_name,\n",
        "        \"view\": explore_name,\n",
        "        \"fields\": fields,\n",
        "        \"filters\": filters,\n",
        "        \"sorts\": sorts,\n",
        "        \"limit\": limit\n",
        "        # Add other WriteQuery parameters here if needed, e.g., \"pivots\": None\n",
        "    }\n",
        "    # print(f\"Constructed query body dictionary for run_inline_query: {query_body_dict}\") # Less verbose\n",
        "\n",
        "\n",
        "    # Execute the inline query\n",
        "    try:\n",
        "        print(f\"Running inline query on {model_name}/{explore_name}...\")\n",
        "        # Pass the dictionary directly as the body parameter\n",
        "        response_json_str = sdk.run_inline_query(result_format=\"json\", body=query_body_dict)\n",
        "        history_data = json.loads(response_json_str)\n",
        "        print(f\"Successfully fetched {len(history_data)} history records via API.\")\n",
        "    except looker_sdk.error.SDKError as e:\n",
        "        print(f\"ERROR: Looker API error fetching history data: {e}\")\n",
        "        # Attempt to print more details from the SDKError if available\n",
        "        if hasattr(e, 'message'): print(f\"       Message: {e.message}\")\n",
        "        if hasattr(e, 'errors'): print(f\"       Errors: {e.errors}\")\n",
        "        print(\"Please ensure the API user has permissions to query system__activity.\")\n",
        "        return None # Return None to indicate failure\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"ERROR: Failed to decode JSON response from Looker API history query: {e}\")\n",
        "        print(f\"Response received (first 500 chars): {response_json_str[:500]}...\") # Log partial response\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Unexpected error fetching history data via API: {e}\")\n",
        "        # import traceback\n",
        "        # print(traceback.format_exc()) # Uncomment for full traceback during debugging\n",
        "        return None\n",
        "\n",
        "    # Process the fetched data\n",
        "    # Use nested defaultdicts: model -> explore -> field -> score\n",
        "    usage_scores = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(float)))\n",
        "    processed_records = 0\n",
        "    skipped_records_parse_error = 0\n",
        "    skipped_records_no_list = 0\n",
        "    skipped_records_zero_counts = 0\n",
        "\n",
        "    if not isinstance(history_data, list):\n",
        "        print(f\"ERROR: Expected a list of records from history API, but received type {type(history_data)}.\")\n",
        "        print(f\"       Data received: {str(history_data)[:500]}...\")\n",
        "        return None # Cannot process non-list data\n",
        "\n",
        "    for record in history_data:\n",
        "        processed_records += 1\n",
        "        # Use keys matching the API response field names\n",
        "        model = record.get(\"query.model\")\n",
        "        explore = record.get(\"query.view\") # Explore name where the query ran\n",
        "        fields_str = record.get(\"query.fields\") # Get the field value (likely a string or list)\n",
        "        source = record.get(\"history.source\")\n",
        "        run_count = record.get(\"history.query_run_count\", 0)\n",
        "        user_count = record.get(\"user.count\", 0) # *** Get user_count ***\n",
        "\n",
        "        # Skip if essential info is missing or counts are 0\n",
        "        if not model or not explore or run_count == 0 or user_count == 0:\n",
        "            skipped_records_zero_counts += 1\n",
        "            continue\n",
        "\n",
        "        fields_list = None\n",
        "        # --- MODIFICATION START: Parse fields_str ---\n",
        "        # Check if fields_str is a non-empty string before trying to parse\n",
        "        if isinstance(fields_str, str) and fields_str.strip() and fields_str.lower() != 'null':\n",
        "            try:\n",
        "                # Attempt to parse the JSON string into a list\n",
        "                parsed_data_field = json.loads(fields_str) # Use different var name\n",
        "                # Ensure the parsed data is actually a list\n",
        "                if isinstance(parsed_data_field, list):\n",
        "                    fields_list = parsed_data_field\n",
        "                else:\n",
        "                     # Log if parsing succeeded but didn't yield a list\n",
        "                     # print(f\"  DEBUG WARNING: Parsed 'query.fields' but result is not a list (type: {type(parsed_data_field)}). Record: {record}\")\n",
        "                     skipped_records_no_list += 1\n",
        "                     continue # Skip if not a list after parsing\n",
        "            except json.JSONDecodeError:\n",
        "                # Log if the string is not valid JSON\n",
        "                # print(f\"  DEBUG WARNING: Failed to JSON decode 'query.fields' string: '{fields_str}'. Record: {record}\")\n",
        "                skipped_records_parse_error += 1\n",
        "                continue # Skip if JSON parsing fails\n",
        "        elif isinstance(fields_str, list):\n",
        "             # Handle case where API *does* return a list directly (more robust)\n",
        "             fields_list = fields_str\n",
        "        else:\n",
        "            # Handle cases where fields_str is None, empty string, the literal string 'null', or not a string/list\n",
        "            skipped_records_no_list += 1\n",
        "            continue # Skip if not a parseable string or list\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "        # Proceed only if fields_list was successfully created as a non-empty list\n",
        "        if not fields_list: # This check covers empty lists as well\n",
        "             skipped_records_no_list += 1 # Count empty lists as skipped\n",
        "             continue\n",
        "\n",
        "        # Get weight for the source\n",
        "        weight = weights.get(source, default_weight) if source else default_weight\n",
        "\n",
        "        # Calculate weighted score incorporating user_count\n",
        "        # Score = Total Runs * Unique Users * Source Weight\n",
        "        query_score_per_field = float(run_count) * float(user_count) * weight\n",
        "\n",
        "        # Add score to each field mentioned in this query record\n",
        "        for field in fields_list:\n",
        "            if isinstance(field, str) and field: # Ensure field is a non-empty string\n",
        "                # Aggregate score: usage_scores[model_name][explore_name][field_name]\n",
        "                usage_scores[model][explore][field] += query_score_per_field\n",
        "\n",
        "    print(f\"Finished processing history data:\")\n",
        "    print(f\"  - Total records processed from API: {processed_records}\")\n",
        "    print(f\"  - Records skipped (zero run/user count): {skipped_records_zero_counts}\")\n",
        "    print(f\"  - Records skipped (field JSON parse error): {skipped_records_parse_error}\")\n",
        "    print(f\"  - Records skipped (no fields/not list/empty): {skipped_records_no_list}\")\n",
        "\n",
        "    # Check if any scores were actually calculated\n",
        "    if not usage_scores:\n",
        "        print(\"WARNING: No field usage scores were calculated. Check history data and processing logic.\")\n",
        "\n",
        "    return usage_scores\n",
        "\n",
        "\n",
        "# --- Prompt Engineering ---\n",
        "def generate_gemini_prompt(model_name, explore_name, explore_lookml_json):\n",
        "    \"\"\"Generates a prompt for Gemini to analyze LookML explore definition for CA readiness.\"\"\"\n",
        "\n",
        "    # Note: Passing the explore_lookml definition as a JSON string.\n",
        "    prompt = f\"\"\"\n",
        "You are an expert LookML developer optimizing Looker Explores specifically for Looker's Conversational Analytics feature (Gemini in Looker). This feature translates natural language questions into Looker API queries based on LookML metadata (fields, labels, descriptions) and data values. Your goal is to evaluate the provided Explore definition for CA readiness and suggest actionable improvements based on CA best practices.\n",
        "\n",
        "**Analyze the following LookML Explore definition:**\n",
        "\n",
        "* **Model:** `{model_name}`\n",
        "* **Explore:** `{explore_name}`\n",
        "\n",
        "**Explore Definition (JSON representation from Looker SDK):**\n",
        "```json\n",
        "{explore_lookml_json}\n",
        "```\n",
        "\n",
        "**Analysis Task:**\n",
        "\n",
        "Evaluate the readiness of this Explore for Conversational Analytics, focusing on common pitfalls and best practices:\n",
        "\n",
        "1.  **Clarity for Natural Language:**\n",
        "    * **Labels:** Are field `label` values clear, concise, business-friendly, and unambiguous? Do they reflect terms users would naturally use? Suggest improved labels where needed.\n",
        "    * **Descriptions:** Are `description` attributes thorough and helpful? **Descriptions are CRITICAL for CA.** They should define the field, provide business context, list synonyms or common terms users might use for this field, and explain calculations if applicable. Identify fields lacking good descriptions or needing more detail/synonyms for CA.\n",
        "    * **Naming Conflicts:** Are there fields with similar names or labels that could cause ambiguity for CA when mapping user questions? Suggest specific ways to resolve ambiguity (e.g., better labels/descriptions, hiding one field).\n",
        "\n",
        "2.  **Field Curation:**\n",
        "    * **Field Bloat:** Does the explore expose too many fields? Are technical fields (Primary Keys, Foreign Keys, intermediate calculations) potentially visible and not hidden?\n",
        "    * **Relevance:** Are the exposed fields relevant for typical conversational questions users might ask of this data? Suggest hiding fields that are irrelevant for conversational use (`hidden: yes`).\n",
        "\n",
        "3.  **Structure & Simplicity:**\n",
        "    * **Joins:** Are the joins relatively simple and logical? Highly complex joins can hinder CA performance. Note if joins seem overly complex.\n",
        "    * **Group Labels:** Are fields logically grouped using `group_label` or `group_item_label` for better organization in the UI, which aids discoverability? Suggest adding group labels where appropriate.\n",
        "\n",
        "4.  **Data Representation:**\n",
        "    * **Data Types:** Are field types (`type`) appropriate for the data (e.g., number, date_*, string, yesno)?\n",
        "    * **Persistent Logic:** Is essential business logic (commonly found in dashboard table calculations or custom fields) defined persistently within LookML dimensions or measures so CA can access it? Identify potential opportunities to convert such logic.\n",
        "\n",
        "**Output Requirements:**\n",
        "\n",
        "Provide your analysis in the following structured format using these exact Markdown headers:\n",
        "\n",
        "### GRADE\n",
        "Assign an overall readiness grade from 0 (Not Ready) to 100 (Excellent). **Output only the integer number.**\n",
        "\n",
        "### RATIONALE\n",
        "Briefly explain the reasoning behind the grade, highlighting key strengths and weaknesses based on the CA best practices above (especially description quality, labeling, field curation, and clarity).\n",
        "\n",
        "### RECOMMENDATIONS\n",
        "Provide a numbered list of specific, actionable recommendations focused on improving this explore *for Conversational Analytics*. Prioritize high-impact changes. Be specific about *which* view/field needs changing and *what* change is needed (e.g., \"Add `description: \\\"Detailed explanation including synonyms...\\\"` to `view.field`.\", \"Add `label: \\\"User Friendly Name\\\"` to `view.field`.\", \"Recommend adding `hidden: yes` to `view.primary_key` in the base view.\", \"Add `group_label: \\\"Group Name\\\"` to related fields like `view.field1`, `view.field2`.\").\n",
        "\n",
        "### GENERATED LOOKML SUGGESTIONS\n",
        "Provide 1-2 concise LookML code snippets demonstrating *how* to implement a key recommendation, focusing on adding descriptions or labels. Example: `dimension: my_field {{ description: \"Detailed explanation...\" }}`. If no specific code changes are easily suggested, state \"No specific LookML snippets suggested based on the provided definition.\"\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# --- Agent Instruction Generation ---\n",
        "def generate_agent_instructions(top_used_fields: list, recommendations: list) -> list:\n",
        "    \"\"\"Generates suggested agent instructions based on analysis results.\"\"\"\n",
        "    instructions = []\n",
        "\n",
        "    # 1. Instruction for most important fields\n",
        "    if top_used_fields:\n",
        "        # Get the names of the top N fields (N defined by AGENT_INSTRUCTION_TOP_FIELDS)\n",
        "        top_field_names = [field_data[0] for field_data in top_used_fields[:AGENT_INSTRUCTION_TOP_FIELDS]]\n",
        "        if top_field_names:\n",
        "            instruction = f\"The most important fields in this data source are likely: {', '.join(top_field_names)}.\"\n",
        "            instructions.append(instruction)\n",
        "\n",
        "    # 2. (Future/Optional) Instruction for fields to avoid\n",
        "    #    Requires more sophisticated parsing of recommendations text.\n",
        "\n",
        "    # 3. (Future/Optional) Instruction for default date field\n",
        "\n",
        "    # 4. (Future/Optional) Instruction for field disambiguation\n",
        "\n",
        "    if not instructions:\n",
        "        instructions.append(\"No specific agent instructions generated automatically. Review recommendations and top fields manually.\")\n",
        "\n",
        "    return instructions\n",
        "\n",
        "# --- LookML Extension File Generation ---\n",
        "def generate_synonyms(field_part: str, label: str) -> list:\n",
        "    \"\"\"Generates potential synonyms based on field name and label.\"\"\"\n",
        "    words = set()\n",
        "    # Split field name by underscore\n",
        "    for word in field_part.split('_'):\n",
        "        if len(word) > 2 and word not in ['id', 'pk', 'fk', 'key', 'date', 'time', 'ts', 'at', 'count', 'sum', 'avg', 'min', 'max', 'p50', 'p90', 'p99']: # Basic stop words\n",
        "            words.add(word.lower())\n",
        "    # Split label by space\n",
        "    if label:\n",
        "        for word in label.split(' '):\n",
        "            cleaned_word = re.sub(r'[^\\w]', '', word) # Remove punctuation\n",
        "            if len(cleaned_word) > 2:\n",
        "                words.add(cleaned_word.lower())\n",
        "\n",
        "    # Basic pluralization/singularization (very naive)\n",
        "    synonyms = set(words)\n",
        "    # for word in words:\n",
        "    #     if word.endswith('s'):\n",
        "    #         synonyms.add(word[:-1])\n",
        "    #     else:\n",
        "    #         synonyms.add(word + 's')\n",
        "\n",
        "    # Return unique, sorted list\n",
        "    return sorted(list(synonyms))\n",
        "\n",
        "\n",
        "def generate_ca_lookml_file_content(explore_key: str, parsed_data: dict, explore_definition_str: str) -> str:\n",
        "    \"\"\"Generates the content for a LookML file containing CA-specific extended views and explore.\"\"\"\n",
        "    model_name, explore_name = explore_key.split('/', 1)\n",
        "    lines = []\n",
        "    indent = \"  \" # Standard LookML indentation\n",
        "    ca_explore_name = explore_name + CA_SUFFIX\n",
        "    view_name_map = {} # Maps original view name to CA extended view name\n",
        "\n",
        "    # --- Helper to safely parse explore definition ---\n",
        "    explore_def = {}\n",
        "    explore_fields_details = {} # Store details like primary_key, description, label\n",
        "    if explore_definition_str and not explore_definition_str.startswith(\"# Error\"):\n",
        "        try:\n",
        "            explore_def = json.loads(explore_definition_str)\n",
        "            # Extract field details for checking primary_key status and getting original desc/label\n",
        "            if 'fields' in explore_def and isinstance(explore_def['fields'], dict):\n",
        "                 for field_type in ['dimensions', 'measures', 'dimension_groups']:\n",
        "                     if field_type in explore_def['fields'] and isinstance(explore_def['fields'][field_type], list):\n",
        "                         for field_detail in explore_def['fields'][field_type]:\n",
        "                             if isinstance(field_detail, dict) and 'name' in field_detail:\n",
        "                                 # Store details using the full field name (view.field) as key\n",
        "                                 explore_fields_details[field_detail['name']] = {\n",
        "                                     'primary_key': field_detail.get('primary_key', False),\n",
        "                                     'description': field_detail.get('description'),\n",
        "                                     'label': field_detail.get('label'),\n",
        "                                     'group_label': field_detail.get('group_label'),\n",
        "                                     'type': field_detail.get('type') # Store type for dim/measure guess\n",
        "                                 }\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"  WARNING: Could not parse explore definition JSON for {explore_key} when generating CA LookML file.\")\n",
        "            explore_def = {} # Use empty dict if parsing fails\n",
        "\n",
        "    # --- Extract view names ---\n",
        "    involved_views = set()\n",
        "    base_view = explore_def.get('view_name')\n",
        "    if base_view:\n",
        "        involved_views.add(base_view)\n",
        "        view_name_map[base_view] = base_view + CA_SUFFIX # Map base view\n",
        "    joins = explore_def.get('joins', [])\n",
        "    original_joins_structure = [] # Store original join details\n",
        "    if isinstance(joins, list):\n",
        "        for join in joins:\n",
        "            # 'name' usually refers to the view being joined\n",
        "            view_from_join = join.get('name')\n",
        "            if view_from_join and isinstance(view_from_join, str):\n",
        "                involved_views.add(view_from_join)\n",
        "                view_name_map[view_from_join] = view_from_join + CA_SUFFIX # Map joined view\n",
        "                original_joins_structure.append(join) # Keep original join details\n",
        "\n",
        "    # --- Prepare context mapping (recommendations/top fields per view/field) ---\n",
        "    view_context = collections.defaultdict(lambda: {'top_fields': [], 'recommendations': collections.defaultdict(list), 'view_recs': []})\n",
        "    # Simple pattern: look for view_name.field_name\n",
        "    # Make pattern more specific to avoid matching things like \"schema.table\"\n",
        "    field_pattern = re.compile(r\"\\b([a-zA-Z0-9_]+)\\.([a-zA-Z0-9_]+)\\b\")\n",
        "\n",
        "    # Map top fields\n",
        "    top_used_fields = parsed_data.get('top_used_fields', [])\n",
        "    for field_name, score in top_used_fields:\n",
        "         match = field_pattern.match(field_name)\n",
        "         if match:\n",
        "             view_name, field_part = match.groups()\n",
        "             # Ensure the view is tracked, even if not in joins (e.g., from base view)\n",
        "             if view_name not in view_name_map: # If view only appeared in top fields\n",
        "                 involved_views.add(view_name)\n",
        "                 view_name_map[view_name] = view_name + CA_SUFFIX\n",
        "             view_context[view_name]['top_fields'].append({'name': field_name, 'field_part': field_part, 'score': score})\n",
        "         # else: field doesn't match view.field pattern, ignore for view mapping\n",
        "\n",
        "    # Map recommendations (best effort)\n",
        "    recommendations = parsed_data.get('recommendations', [])\n",
        "    explore_recommendations = []\n",
        "    if recommendations: # Check if recommendations exist\n",
        "        for rec in recommendations:\n",
        "            matches = field_pattern.findall(rec)\n",
        "            mapped_to_field = False\n",
        "            if matches:\n",
        "                # Try mapping to all fields mentioned\n",
        "                for view_name, field_part in matches:\n",
        "                    if view_name in involved_views: # Only map if the view is relevant to this explore\n",
        "                        view_context[view_name]['recommendations'][field_part].append(rec)\n",
        "                        mapped_to_field = True\n",
        "            # If not mapped to a specific field, check if it mentions a view\n",
        "            if not mapped_to_field:\n",
        "                found_view = False\n",
        "                for view_name in involved_views:\n",
        "                     # Use word boundary to avoid partial matches (e.g., 'user' in 'user_id')\n",
        "                     if re.search(rf'\\b{re.escape(view_name)}\\b', rec):\n",
        "                         view_context[view_name]['view_recs'].append(rec)\n",
        "                         found_view = True\n",
        "                         # Don't break, recommendation might apply to multiple views\n",
        "                if not found_view:\n",
        "                     explore_recommendations.append(rec) # Keep as explore-level rec\n",
        "\n",
        "\n",
        "    # --- Generate File Content ---\n",
        "    # Header Comments & Connection/Include Placeholders\n",
        "    lines.append(f\"# LookML File for CA-Optimized Explore: {explore_key}\")\n",
        "    lines.append(f\"# Generated by Conversational Readiness Analyzer Script on {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    lines.append(\"#\")\n",
        "    lines.append(\"# Purpose: This file defines extended Views and a new Explore based on\")\n",
        "    lines.append(f\"#          '{explore_name}', curated for Conversational Analytics (CA).\")\n",
        "    lines.append(\"#\")\n",
        "    lines.append(\"# Instructions:\")\n",
        "    lines.append(\"# 1. Save this file in your LookML project (e.g., 'ca_extensions/{explore_name}_ca.explore.lkml').\")\n",
        "    lines.append(\"# 2. Replace 'CONNECTION_NAME_PLACEHOLDER' with your actual connection name.\")\n",
        "    lines.append(\"# 3. Verify the 'include:' paths point correctly to your original view files.\")\n",
        "    lines.append(\"# 4. Include this file in your model file (e.g., include: \\\"/ca_extensions/*.explore.lkml\\\")\")\n",
        "    lines.append(\"# 5. Review and refine the auto-generated labels and descriptions below.\")\n",
        "    lines.append(\"# 6. IMPORTANT: Add 'fields_hidden_by_default: yes' to the *original base view files*\")\n",
        "    lines.append(\"#    referenced via 'extends' to ensure only explicitly unhidden fields are exposed in the CA explore.\")\n",
        "    lines.append(\"#\")\n",
        "    lines.append(\"connection: \\\"CONNECTION_NAME_PLACEHOLDER\\\"\")\n",
        "    # Include original views needed for extension\n",
        "    include_paths = set()\n",
        "    for view_name in involved_views:\n",
        "        # Attempt a generic path - USER MUST VERIFY THIS\n",
        "        include_paths.add(f\"/views/{view_name}.view.lkml\") # Common pattern, but needs verification\n",
        "    for path in sorted(list(include_paths)):\n",
        "         lines.append(f\"include: \\\"{path}\\\" # Verify this path is correct\")\n",
        "\n",
        "    lines.append(\"\\n\")\n",
        "    lines.append(\"# --- Analysis Summary (For Context) ---\")\n",
        "    lines.append(f\"# Grade: {parsed_data.get('grade', 'N/A')}/100\")\n",
        "    lines.append(\"# Rationale:\")\n",
        "    rationale = parsed_data.get('rationale', 'N/A')\n",
        "    rationale_lines = textwrap.wrap(rationale if rationale else 'N/A', width=80, initial_indent=\"#   \", subsequent_indent=\"#   \")\n",
        "    lines.extend(rationale_lines)\n",
        "    lines.append(\"# Top Used Fields (Weighted Score):\")\n",
        "    if top_used_fields:\n",
        "        for field, score in top_used_fields:\n",
        "            lines.append(f\"#   - {field} ({score})\")\n",
        "    else:\n",
        "        lines.append(\"#   N/A\")\n",
        "    lines.append(\"# ------------------------------------\")\n",
        "    lines.append(\"\\n\")\n",
        "\n",
        "    # Extended View Blocks\n",
        "    if not involved_views:\n",
        "         lines.append(\"# WARNING: Could not identify views associated with this explore from the definition.\")\n",
        "    else:\n",
        "        lines.append(\"# --- Extended Views for CA ---\")\n",
        "        lines.append(\"# These views extend the original views and contain CA-specific refinements.\")\n",
        "        lines.append(\"# Add 'fields_hidden_by_default: yes' to original views for best practice.\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "        processed_fields_in_view = collections.defaultdict(set)\n",
        "\n",
        "        for view_name in sorted(list(involved_views)):\n",
        "            ca_view_name = view_name_map.get(view_name, view_name + CA_SUFFIX) # Get CA view name\n",
        "            lines.append(f\"view: {ca_view_name} extends: [{view_name}] {{\")\n",
        "            lines.append(f\"{indent}# IMPORTANT: Consider adding 'fields_hidden_by_default: yes' to the original '{view_name}.view.lkml' file.\")\n",
        "            lines.append(\"\")\n",
        "            context = view_context[view_name]\n",
        "            has_content = False # Track if view refinement has actionable items\n",
        "\n",
        "            # Add view-level recommendations (concise)\n",
        "            if context['view_recs']:\n",
        "                has_content = True\n",
        "                lines.append(indent + \"# General Recommendations for this view:\")\n",
        "                for i, rec in enumerate(context['view_recs']):\n",
        "                     # Keep recommendations concise here\n",
        "                     lines.append(f\"{indent}# - {rec[:100]}{'...' if len(rec)>100 else ''}\") # Truncate long recs\n",
        "                lines.append(\"\") # Blank line after view recs\n",
        "\n",
        "            # Combine top fields and other recommended fields for processing\n",
        "            fields_to_process = {}\n",
        "            for field_info in context['top_fields']:\n",
        "                fields_to_process[field_info['field_part']] = field_info\n",
        "            for field_part in context['recommendations']:\n",
        "                 if field_part not in fields_to_process:\n",
        "                     # Find original field name if possible\n",
        "                     field_name = f\"{view_name}.{field_part}\"\n",
        "                     fields_to_process[field_part] = {'name': field_name, 'field_part': field_part, 'score': None} # Score is None if not a top field\n",
        "\n",
        "            if fields_to_process:\n",
        "                has_content = True\n",
        "                lines.append(indent + \"# Field Refinements (Focus on Top Fields & Recommendations):\")\n",
        "                # Process fields, prioritizing top fields shown first\n",
        "                sorted_fields_to_process = sorted(\n",
        "                    fields_to_process.values(),\n",
        "                    key=lambda x: x['score'] if x['score'] is not None else -1, # Sort top fields first\n",
        "                    reverse=True\n",
        "                )\n",
        "\n",
        "                for field_info in sorted_fields_to_process:\n",
        "                    field_part = field_info['field_part']\n",
        "                    field_name = field_info['name'] # Full name like view.field\n",
        "                    score = field_info['score']\n",
        "                    original_details = explore_fields_details.get(field_name, {})\n",
        "                    is_pk = original_details.get('primary_key', False)\n",
        "                    original_label = original_details.get('label')\n",
        "                    original_description = original_details.get('description')\n",
        "                    original_group_label = original_details.get('group_label')\n",
        "                    field_type_guess = \"measure\" if any(x in field_part for x in ['count', 'sum', 'average', 'total', 'min', 'max', 'percent', 'ratio', '_revenue', '_amount']) else \"dimension\"\n",
        "\n",
        "                    lines.append(f\"{indent}# --- Field: {field_name} {'(Score: ' + str(score) + ')' if score is not None else ''} ---\")\n",
        "\n",
        "                    # Add concise comment if specific recommendations exist\n",
        "                    field_recs = context['recommendations'].get(field_part, [])\n",
        "                    if field_recs:\n",
        "                        lines.append(f\"{indent}# ACTION: Review Gemini recommendations for this field (see full list below view).\")\n",
        "\n",
        "                    lines.append(f\"{indent}{field_type_guess}: {field_part} {{\")\n",
        "                    if is_pk:\n",
        "                         lines.append(f\"{indent}{indent}# This is a primary key. Should likely remain hidden in the base view.\")\n",
        "                         lines.append(f\"{indent}{indent}hidden: yes # Ensure hidden in base or uncomment to explicitly hide here.\")\n",
        "                    else:\n",
        "                         lines.append(f\"{indent}{indent}hidden: no # Explicitly unhide for CA\")\n",
        "                         # Attempt to auto-populate label\n",
        "                         default_label = field_part.replace('_',' ').title()\n",
        "                         suggested_label = original_label if original_label else default_label\n",
        "                         lines.append(f\"{indent}{indent}# Verify/Refine user-friendly label.\")\n",
        "                         lines.append(f\"{indent}{indent}label: \\\"{suggested_label}\\\"\")\n",
        "                         # Attempt to auto-populate description with synonyms\n",
        "                         description_parts = []\n",
        "                         if original_description:\n",
        "                             description_parts.append(original_description)\n",
        "                         # Generate synonyms\n",
        "                         synonyms = generate_synonyms(field_part, suggested_label)\n",
        "                         if synonyms:\n",
        "                             description_parts.append(f\"(Synonyms: {', '.join(synonyms)})\")\n",
        "\n",
        "                         suggested_description = \" \".join(description_parts).strip()\n",
        "                         if not suggested_description: # Handle empty case\n",
        "                             suggested_description = f\"Description for {field_name}\"\n",
        "\n",
        "                         lines.append(f\"{indent}{indent}# Verify/Refine detailed description for CA context, synonyms & calculations.\")\n",
        "                         lines.append(f\"{indent}{indent}description: \\\"{suggested_description}\\\"\")\n",
        "                         # Group Label\n",
        "                         lines.append(f\"{indent}{indent}# Assign group_label if applicable for better UI grouping\")\n",
        "                         lines.append(f\"{indent}{indent}group_label: \\\"{original_group_label if original_group_label else ''}\\\"\")\n",
        "\n",
        "                    lines.append(f\"{indent}}}\")\n",
        "                    lines.append(\"\") # Blank line after field block\n",
        "\n",
        "            if not has_content:\n",
        "                 lines.append(indent + \"# No specific top fields or recommendations identified for this view.\")\n",
        "                 lines.append(indent + \"# Add manual refinements if needed.\")\n",
        "\n",
        "            # List full recommendations for this view at the end for reference\n",
        "            if any(context['recommendations'].values()):\n",
        "                 lines.append(indent + \"# Full Recommendations Mentioning Fields in this View:\")\n",
        "                 for field_part, recs in context['recommendations'].items():\n",
        "                     for i, rec in enumerate(recs):\n",
        "                          rec_lines = textwrap.wrap(f\"{view_name}.{field_part}: {rec}\", width=70, initial_indent=f\"{indent}# \", subsequent_indent=f\"{indent}#   \")\n",
        "                          lines.extend(rec_lines)\n",
        "                 lines.append(\"\")\n",
        "\n",
        "\n",
        "            lines.append(\"}}\") # Close view block\n",
        "            lines.append(\"\\n\")\n",
        "\n",
        "    # New Explore Definition using Extended Views\n",
        "    lines.append(\"\\n\" + \"-\"*30 + \"\\n\")\n",
        "    lines.append(\"# --- CA-Optimized Explore Definition ---\")\n",
        "    lines.append(f\"explore: {ca_explore_name} {{\")\n",
        "    lines.append(f\"{indent}# This explore uses the extended *_ca views defined above.\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(indent + \"# Add a user-friendly label for conversational use.\")\n",
        "    lines.append(indent + f\"label: \\\"{explore_def.get('label', explore_name)} (Conversational)\\\" # Example label\")\n",
        "    lines.append(indent + \"# Add a clear description explaining the explore's purpose for CA.\")\n",
        "    lines.append(indent + f\"description: \\\"{explore_def.get('description', '')} (Optimized for Conversational Analytics)\\\"\") # Seed with original desc\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Define the 'from' using the extended base view\n",
        "    ca_base_view = view_name_map.get(base_view)\n",
        "    if ca_base_view:\n",
        "        lines.append(f\"{indent}from: {ca_base_view}\")\n",
        "    else:\n",
        "        lines.append(f\"{indent}# ERROR: Could not determine base view for explore '{explore_name}'.\")\n",
        "\n",
        "    # Recreate joins using extended view names\n",
        "    if original_joins_structure:\n",
        "        lines.append(\"\")\n",
        "        lines.append(f\"{indent}# Joins using *_ca extended views\") # Changed comment style\n",
        "        for join in original_joins_structure:\n",
        "            original_join_view = join.get('name')\n",
        "            ca_join_view = view_name_map.get(original_join_view)\n",
        "            # Determine the 'from' view for the join - defaults to base view if not specified in join def\n",
        "            original_from_view = join.get('from', base_view)\n",
        "            ca_from_view = view_name_map.get(original_from_view)\n",
        "\n",
        "            if ca_join_view and ca_from_view: # Only add join if both views could be mapped\n",
        "                lines.append(f\"{indent}join: {ca_join_view} {{\")\n",
        "                # Copy essential join parameters\n",
        "                if join.get('type'): lines.append(f\"{indent}{indent}type: {join['type']}\")\n",
        "                if join.get('relationship'): lines.append(f\"{indent}{indent}relationship: {join['relationship']}\")\n",
        "                if join.get('sql_on'): lines.append(f\"{indent}{indent}sql_on: {join['sql_on']} ;;\")\n",
        "                # Reference the extended 'from' view\n",
        "                lines.append(f\"{indent}{indent}from: {ca_from_view}\")\n",
        "                # Copy view_label if it existed\n",
        "                if join.get('view_label'): lines.append(f\"{indent}{indent}view_label: \\\"{join['view_label']}\\\"\")\n",
        "                lines.append(f\"{indent}}}\")\n",
        "            else:\n",
        "                lines.append(f\"{indent}# WARNING: Could not map original join for '{original_join_view}'. Original join details:\")\n",
        "                lines.append(f\"{indent}# {json.dumps(join)}\")\n",
        "\n",
        "\n",
        "    lines.append(\"}}\") # Close explore block\n",
        "    lines.append(\"\\n\")\n",
        "\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# --- Looker Interaction ---\n",
        "def get_all_explores(sdk):\n",
        "    \"\"\"Fetches all non-hidden explores from the Looker instance.\"\"\"\n",
        "    all_explores_list = []\n",
        "    print(\"Fetching LookML models and their explores from Looker...\")\n",
        "    try:\n",
        "        # Request fields including explores and ensure we only get models with explores\n",
        "        # Also fetch explore label and description if available\n",
        "        models = sdk.all_lookml_models(fields=\"name,project_name,explores(name,hidden,label,description)\")\n",
        "        if not models:\n",
        "             print(\"WARNING: No LookML models found or returned by the SDK. Check Looker permissions or if models exist.\")\n",
        "             return []\n",
        "\n",
        "        model_count = 0\n",
        "        explore_count = 0\n",
        "        for model in models:\n",
        "            model_count += 1\n",
        "            # Ensure model has a name and explores attribute before iterating\n",
        "            if model.name and model.explores:\n",
        "                for explore in model.explores:\n",
        "                    # Include only explores that have a name and are not hidden\n",
        "                    if explore.name and not explore.hidden:\n",
        "                        # Store as a tuple (model_name, explore_name)\n",
        "                        all_explores_list.append((model.name, explore.name))\n",
        "                        explore_count += 1\n",
        "            elif model.name:\n",
        "                 print(f\"  Info: Model '{model.name}' has no explores defined or returned.\")\n",
        "            else:\n",
        "                 print(\"  Warning: Encountered a model without a name.\")\n",
        "\n",
        "\n",
        "        print(f\"Successfully scanned {model_count} models and found {explore_count} non-hidden explores.\")\n",
        "        if not all_explores_list:\n",
        "             print(\"WARNING: No non-hidden explores were found across all models.\")\n",
        "        return all_explores_list\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed while fetching models or explores from Looker: {e}\")\n",
        "        print(\"Check SDK authentication and permissions (user needs access to see models and explores).\")\n",
        "        return [] # Return empty list on error\n",
        "\n",
        "def get_explore_lookml_definition(sdk, model_name, explore_name):\n",
        "    \"\"\"\n",
        "    Fetches the detailed LookML definition of a specific explore using the SDK.\n",
        "    Returns a JSON string representation of the explore details.\n",
        "    \"\"\"\n",
        "    print(f\"  Fetching definition for explore: {model_name}/{explore_name}...\")\n",
        "    try:\n",
        "        # Fetch detailed explore information. Request fields relevant for analysis.\n",
        "        # Make sure to request 'view_name' and 'joins(name, type, relationship, sql_on, from, view_label)' to identify views and reconstruct joins\n",
        "        # Also fetch primary_key status for dimensions\n",
        "        explore_details = sdk.lookml_model_explore(\n",
        "            lookml_model_name=model_name,\n",
        "            explore_name=explore_name,\n",
        "            # Request fields needed for view identification and analysis\n",
        "            # Ensure we get original label/description/group_label for fields\n",
        "            fields=\"\"\"\n",
        "                name, label, description, hidden, group_label, view_name,\n",
        "                joins(name, view_label, from, relationship, type, sql_on),\n",
        "                fields(\n",
        "                    dimensions(name, label, label_short, description, type, sql, primary_key, hidden, group_label, group_item_label),\n",
        "                    measures(name, label, label_short, description, type, sql, hidden, group_label, group_item_label),\n",
        "                    dimension_groups(name, label, label_short, description, type, timeframes, sql, hidden, group_label, group_item_label),\n",
        "                    filters(name, label, label_short, description, type, hidden, group_label, group_item_label),\n",
        "                    parameters(name, label, label_short, description, type, hidden, group_label, group_item_label)\n",
        "                ),\n",
        "                always_filter, conditionally_filter, index_fields\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Convert the SDK object to JSON using a default handler for object attributes.\n",
        "        try:\n",
        "            # Use default=vars might be slightly cleaner than lambda o: o.__dict__ sometimes\n",
        "            explore_definition_json = json.dumps(explore_details, default=vars, indent=2)\n",
        "        except TypeError as json_error:\n",
        "             # Fallback if vars doesn't work\n",
        "             try:\n",
        "                 explore_definition_json = json.dumps(explore_details, default=lambda o: o.__dict__, indent=2)\n",
        "             except TypeError as json_error_fallback:\n",
        "                 print(f\"  ERROR: Failed to serialize Looker SDK object to JSON (tried vars and __dict__): {json_error_fallback}\")\n",
        "                 print(f\"  Object type: {type(explore_details)}\")\n",
        "                 return f\"# Error: Failed to serialize explore details for {model_name}/{explore_name}: {json_error_fallback}\"\n",
        "\n",
        "        if not explore_definition_json or explore_definition_json == \"{}\":\n",
        "             print(f\"  Warning: Could not retrieve a valid definition for {model_name}/{explore_name}. API returned empty details or serialization failed.\")\n",
        "             return f\"# Error: Could not retrieve valid definition for {model_name}/{explore_name}\"\n",
        "\n",
        "        print(f\"  Successfully fetched and formatted definition for {model_name}/{explore_name}.\")\n",
        "        return explore_definition_json\n",
        "\n",
        "    except looker_sdk.error.SDKError as looker_error:\n",
        "        if looker_error.status == 404:\n",
        "             print(f\"  ERROR: Explore '{model_name}/{explore_name}' not found (404). Skipping.\")\n",
        "             return f\"# Error: Explore not found (404) for {model_name}/{explore_name}\"\n",
        "        else:\n",
        "             print(f\"  ERROR: Looker SDK error fetching definition for '{model_name}/{explore_name}': {looker_error}\")\n",
        "             return f\"# Error: Looker SDK error ({looker_error.status}) for {model_name}/{explore_name}: {looker_error.message}\"\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR: Unexpected error fetching LookML definition for explore '{explore_name}' in model '{model_name}': {e}\")\n",
        "        return f\"# Error: Unexpected error fetching definition for {model_name}/{explore_name}: {e}\"\n",
        "\n",
        "\n",
        "# --- Gemini Interaction ---\n",
        "def analyze_with_gemini(model, prompt, model_name, explore_name):\n",
        "    \"\"\"Sends a prompt to the Gemini model and returns the response text.\"\"\"\n",
        "    print(f\"  Sending request to Gemini ({gemini_model_name}) for analysis of {model_name}/{explore_name}...\")\n",
        "    try:\n",
        "        # Configure safety settings and generation parameters\n",
        "        generation_config = generative_models.GenerationConfig(\n",
        "            max_output_tokens=8192,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            top_k=40\n",
        "        )\n",
        "        safety_settings = {\n",
        "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "        }\n",
        "\n",
        "        gemini_model_instance = model\n",
        "\n",
        "        response = gemini_model_instance.generate_content(\n",
        "            [prompt],\n",
        "            generation_config=generation_config,\n",
        "            safety_settings=safety_settings,\n",
        "            stream=False,\n",
        "        )\n",
        "\n",
        "        print(f\"  Received response from Gemini for {model_name}/{explore_name}.\")\n",
        "\n",
        "        # --- Enhanced Response Validation ---\n",
        "        if not response.candidates:\n",
        "             print(\"  ERROR: Gemini response did not contain any candidates.\")\n",
        "             return \"Error: No response candidates returned by Gemini.\"\n",
        "\n",
        "        candidate = response.candidates[0]\n",
        "\n",
        "        if candidate.finish_reason != FinishReason.STOP:\n",
        "            finish_reason_name = candidate.finish_reason.name\n",
        "            print(f\"  WARNING: Gemini response finished with reason: {finish_reason_name}\")\n",
        "            if candidate.finish_reason == FinishReason.SAFETY:\n",
        "                 print(\"  ERROR: Response was blocked by Gemini's safety filters.\")\n",
        "                 return f\"Error: Gemini response blocked due to safety filters for {model_name}/{explore_name}.\"\n",
        "            elif candidate.finish_reason == FinishReason.MAX_TOKENS:\n",
        "                 print(\"  ERROR: Response may be truncated because maximum output tokens were reached.\")\n",
        "                 if candidate.content and candidate.content.parts:\n",
        "                     return f\"Error: Response truncated (MAX_TOKENS). Analysis might be incomplete.\\n\\n{candidate.content.parts[0].text}\"\n",
        "                 else:\n",
        "                     return \"Error: Response truncated (MAX_TOKENS) and no content returned.\"\n",
        "            else:\n",
        "                 return f\"Error: Gemini generation stopped unexpectedly ({finish_reason_name}) for {model_name}/{explore_name}.\"\n",
        "\n",
        "        if not candidate.content or not candidate.content.parts:\n",
        "             print(\"  ERROR: Gemini response candidate does not contain content parts.\")\n",
        "             return f\"Error: No text content found in Gemini response for {model_name}/{explore_name}.\"\n",
        "\n",
        "        generated_text = candidate.content.parts[0].text\n",
        "        print(f\"  Successfully extracted analysis text from Gemini response.\")\n",
        "        return generated_text\n",
        "        # --- End Enhanced Response Validation ---\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR: Unexpected error interacting with Gemini API for {model_name}/{explore_name}: {e}\")\n",
        "        return f\"Error: Unexpected error during Gemini analysis for {model_name}/{explore_name}: {e}\"\n",
        "\n",
        "# --- Main Script Logic ---\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate the LookML explore analysis.\"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(\"--- Starting LookML Conversational Readiness Analysis ---\")\n",
        "    print(f\"Script started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # 1. Get Looker Config from Secret Manager\n",
        "    print(\"\\nStep 1: Fetching Looker credentials...\")\n",
        "    looker_config = get_looker_config_from_secret_manager(\n",
        "        looker_project_id, looker_secret_name, looker_secret_version\n",
        "    )\n",
        "\n",
        "    # 2. Initialize Looker SDK\n",
        "    print(\"\\nStep 2: Initializing Looker SDK...\")\n",
        "    sdk = initialize_looker_sdk(looker_config)\n",
        "\n",
        "    # 3. Initialize Vertex AI\n",
        "    print(\"\\nStep 3: Initializing Vertex AI...\")\n",
        "    initialize_vertex_ai(vertex_project_id, vertex_location)\n",
        "    # Instantiate the Gemini model to be used for analysis\n",
        "    try:\n",
        "        print(f\"Instantiating Gemini model: {gemini_model_name}...\")\n",
        "        gemini_model = GenerativeModel(gemini_model_name)\n",
        "        print(\"Gemini model instantiated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to instantiate Gemini model '{gemini_model_name}': {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 4. Fetch and Process History Data via API (Do this after SDK is initialized)\n",
        "    print(\"\\nStep 4: Fetching and Processing Looker History Data via API...\")\n",
        "    history_scores = fetch_and_process_history(sdk, SOURCE_WEIGHTS, DEFAULT_WEIGHT)\n",
        "    if history_scores is None:\n",
        "        print(\"WARNING: Proceeding without field usage analysis due to history fetching/processing error.\")\n",
        "        history_scores = {} # Use an empty dict to avoid errors later\n",
        "\n",
        "\n",
        "    # 5. Determine Explores to Analyze\n",
        "    print(\"\\nStep 5: Determining explores to analyze...\")\n",
        "    explores_to_analyze = []\n",
        "    if TARGET_EXPLORES:\n",
        "        # Use specified target explores\n",
        "        print(f\"  Using the {len(TARGET_EXPLORES)} specified target explore(s).\")\n",
        "        for target in TARGET_EXPLORES:\n",
        "            parts = target.split('/')\n",
        "            if len(parts) == 2:\n",
        "                explores_to_analyze.append((parts[0].strip(), parts[1].strip())) # Add stripped parts\n",
        "            else:\n",
        "                print(f\"  WARNING: Skipping invalid target explore format: '{target}'. Use 'model_name/explore_name'.\")\n",
        "        if not explores_to_analyze:\n",
        "             print(\"ERROR: TARGET_EXPLORES list was provided, but no valid explores could be parsed. Exiting.\")\n",
        "             sys.exit(1)\n",
        "    else:\n",
        "        # Get All Explores from Looker\n",
        "        print(\"  TARGET_EXPLORES is empty. Fetching all non-hidden explores from Looker...\")\n",
        "        explores_to_analyze = get_all_explores(sdk)\n",
        "        if not explores_to_analyze:\n",
        "            print(\"No explores found or error occurred while fetching. Exiting script.\")\n",
        "            return # Exit gracefully if no explores\n",
        "\n",
        "    total_explores = len(explores_to_analyze)\n",
        "    print(f\"Prepared {total_explores} explores for analysis.\")\n",
        "\n",
        "\n",
        "    # 6. Analyze Each Explore\n",
        "    print(f\"\\nStep 6: Starting analysis loop for {total_explores} explores...\")\n",
        "    results = {} # Dictionary to store analysis results for each explore\n",
        "    processed_count = 0\n",
        "\n",
        "    for model_name, explore_name in explores_to_analyze:\n",
        "        processed_count += 1\n",
        "        explore_key = f\"{model_name}/{explore_name}\"\n",
        "        print(f\"\\n--- Analyzing Explore {processed_count}/{total_explores}: {explore_key} ---\")\n",
        "        explore_start_time = time.time()\n",
        "        # Store results for this explore, including the definition needed later\n",
        "        current_result = {\"explore_definition_json\": None}\n",
        "\n",
        "        # Get LookML definition (structure) for the current explore\n",
        "        explore_definition_json = get_explore_lookml_definition(sdk, model_name, explore_name)\n",
        "        current_result[\"explore_definition_json\"] = explore_definition_json # Store for later use\n",
        "\n",
        "        # Check if fetching the definition failed\n",
        "        if explore_definition_json.startswith(\"# Error\"):\n",
        "            print(f\"  Skipping analysis for {explore_key} due to error fetching definition.\")\n",
        "            current_result.update({\"status\": \"Fetch Error\", \"error\": explore_definition_json, \"analysis\": None, \"top_used_fields\": []})\n",
        "            results[explore_key] = current_result\n",
        "            continue # Move to the next explore\n",
        "\n",
        "        # Generate the specific prompt for this explore\n",
        "        prompt = generate_gemini_prompt(model_name, explore_name, explore_definition_json)\n",
        "\n",
        "        # Call Gemini for analysis\n",
        "        gemini_response_text = analyze_with_gemini(gemini_model, prompt, model_name, explore_name)\n",
        "\n",
        "        # Store Gemini analysis result\n",
        "        if gemini_response_text.startswith(\"Error:\"):\n",
        "             current_result.update({\"status\": \"Analysis Error\", \"error\": gemini_response_text, \"analysis\": None})\n",
        "             print(f\"  Analysis FAILED for {explore_key}.\")\n",
        "        else:\n",
        "             current_result.update({\"status\": \"Analysis Success\", \"error\": None, \"analysis\": gemini_response_text})\n",
        "             print(f\"  Analysis SUCCEEDED for {explore_key}.\")\n",
        "\n",
        "        # Get Top Used Fields from processed history data\n",
        "        # Use the explore_name from the loop which matches the explore definition fetched\n",
        "        explore_usage_scores = history_scores.get(model_name, {}).get(explore_name, {})\n",
        "        if explore_usage_scores:\n",
        "            sorted_fields = sorted(explore_usage_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            # Format as [field_name, score] pairs, rounded for readability\n",
        "            top_fields = [[field, round(score, 2)] for field, score in sorted_fields[:TOP_N_FIELDS]]\n",
        "            current_result[\"top_used_fields\"] = top_fields\n",
        "            print(f\"  Found {len(top_fields)} top used fields based on history.\")\n",
        "        else:\n",
        "            current_result[\"top_used_fields\"] = []\n",
        "            print(f\"  No usage history found for {explore_key}.\")\n",
        "\n",
        "        # Store results for this explore\n",
        "        results[explore_key] = current_result\n",
        "\n",
        "        explore_end_time = time.time()\n",
        "        print(f\"  Time taken for {explore_key}: {explore_end_time - explore_start_time:.2f} seconds\")\n",
        "\n",
        "        # Optional: Add delay between API calls\n",
        "        if API_CALL_DELAY > 0 and processed_count < total_explores:\n",
        "            print(f\"  Waiting for {API_CALL_DELAY} second(s) before next API call...\")\n",
        "            time.sleep(API_CALL_DELAY)\n",
        "\n",
        "    print(\"\\n--- Analysis Loop Complete ---\")\n",
        "\n",
        "    # 7. Process and Output Results (Parse and Summarize)\n",
        "    print(\"\\nStep 7: Processing and Outputting Results...\")\n",
        "\n",
        "    fetch_errors = 0\n",
        "    analysis_errors = 0\n",
        "    analysis_success = 0\n",
        "    parsing_errors = 0\n",
        "    parsed_results = {} # Store structured results after parsing\n",
        "\n",
        "    for key, result_data in results.items():\n",
        "        print(f\"\\n--- Processing Results for {key} ---\")\n",
        "        status = result_data[\"status\"]\n",
        "        error = result_data[\"error\"]\n",
        "        analysis_text = result_data[\"analysis\"]\n",
        "        top_used_fields = result_data.get(\"top_used_fields\", [])\n",
        "        explore_definition_json = result_data.get(\"explore_definition_json\") # Get stored definition\n",
        "\n",
        "        # Start with basic info + top fields\n",
        "        parsed_data = {\"status\": status, \"error\": error, \"top_used_fields\": top_used_fields}\n",
        "        # Initialize fields for parsed analysis results\n",
        "        parsed_data[\"grade\"] = None\n",
        "        parsed_data[\"rationale\"] = None\n",
        "        parsed_data[\"recommendations\"] = []\n",
        "        parsed_data[\"generated_lookml_suggestions\"] = None\n",
        "        parsed_data[\"suggested_agent_instructions\"] = []\n",
        "        parsed_data[\"suggested_ca_lookml_file\"] = None # Changed key name\n",
        "\n",
        "        recommendations = [] # Local variable for recommendations\n",
        "\n",
        "        if status == \"Fetch Error\":\n",
        "            fetch_errors += 1\n",
        "            print(f\"  Status: {status}\")\n",
        "            print(f\"  Error Details: {error}\")\n",
        "        elif status == \"Analysis Error\":\n",
        "            analysis_errors += 1\n",
        "            print(f\"  Status: {status}\")\n",
        "            print(f\"  Error Details: {error}\")\n",
        "        elif status == \"Analysis Success\":\n",
        "            # Attempt to parse the structured output from the successful analysis\n",
        "            try:\n",
        "                # Use Markdown headers for splitting\n",
        "                grade_str = analysis_text.split(\"### GRADE\")[1].split(\"###\")[0].strip()\n",
        "                rationale = analysis_text.split(\"### RATIONALE\")[1].split(\"###\")[0].strip()\n",
        "                recommendations_raw = analysis_text.split(\"### RECOMMENDATIONS\")[1].split(\"###\")[0].strip()\n",
        "                suggestions = analysis_text.split(\"### GENERATED LOOKML SUGGESTIONS\")[1].strip()\n",
        "\n",
        "                # Convert grade to int\n",
        "                try:\n",
        "                    grade = int(grade_str)\n",
        "                except ValueError:\n",
        "                    print(f\"  Warning: Could not parse grade '{grade_str}' as integer for {key}.\")\n",
        "                    grade = None\n",
        "\n",
        "                # Split recommendations into a list\n",
        "                recommendations = [\n",
        "                    rec.strip().lstrip('0123456789. ')\n",
        "                    for rec in recommendations_raw.split('\\n')\n",
        "                    if rec.strip() and (rec.strip()[0].isdigit() or rec.strip()[0] == '-') # Allow numbered or dashed lists\n",
        "                ]\n",
        "\n",
        "                # Update parsed_data with parsed Gemini analysis\n",
        "                parsed_data.update({\n",
        "                    \"grade\": grade,\n",
        "                    \"rationale\": rationale,\n",
        "                    \"recommendations\": recommendations,\n",
        "                    \"generated_lookml_suggestions\": suggestions,\n",
        "                    # \"raw_analysis\": analysis_text # Optionally keep raw text\n",
        "                })\n",
        "                print(f\"  Parsing: SUCCESSFUL (Grade: {grade}, Recommendations: {len(recommendations)})\")\n",
        "                analysis_success += 1 # Increment success count here\n",
        "\n",
        "            except IndexError:\n",
        "                 print(f\"  ERROR: Failed to parse structured output for {key}. Required headers missing or malformed.\")\n",
        "                 parsing_errors += 1\n",
        "                 parsed_data[\"status\"] = \"Parsing Error\" # Update status\n",
        "                 parsed_data[\"error\"] = \"Failed to find expected Markdown headers in analysis text.\"\n",
        "                 parsed_data[\"raw_analysis\"] = analysis_text # Store raw text for debugging\n",
        "            except Exception as parse_error:\n",
        "                print(f\"  ERROR: Unexpected error parsing structured output for {key}: {parse_error}\")\n",
        "                parsing_errors += 1\n",
        "                parsed_data[\"status\"] = \"Parsing Error\" # Update status\n",
        "                parsed_data[\"error\"] = f\"Unexpected parsing error: {parse_error}\"\n",
        "                parsed_data[\"raw_analysis\"] = analysis_text # Store raw text\n",
        "\n",
        "        else: # Handle unknown status\n",
        "             print(f\"  Status: UNKNOWN ({status})\")\n",
        "             parsed_data[\"status\"] = \"Unknown Status\"\n",
        "\n",
        "        # --- Generate Agent Instructions (even if parsing failed, use top fields) ---\n",
        "        generated_instructions = generate_agent_instructions(top_used_fields, recommendations)\n",
        "        parsed_data[\"suggested_agent_instructions\"] = generated_instructions\n",
        "        # --- Generate LookML Extension File Content ---\n",
        "        # Pass the explore definition string needed for view extraction\n",
        "        # Use the new function name generate_ca_lookml_file_content\n",
        "        ca_lookml_content = generate_ca_lookml_file_content(key, parsed_data, explore_definition_json)\n",
        "        parsed_data[\"suggested_ca_lookml_file\"] = ca_lookml_content # Changed key name\n",
        "\n",
        "        if status != \"Fetch Error\": # Only print generation messages if analysis was attempted\n",
        "             print(f\"  Generated {len(generated_instructions)} agent instruction(s).\")\n",
        "             print(f\"  Generated suggested CA LookML file content.\")\n",
        "        # --- End Generation ---\n",
        "\n",
        "        # Add the potentially updated parsed_data to final results\n",
        "        parsed_results[key] = parsed_data\n",
        "        print(f\"  Top Used Fields Found: {len(top_used_fields)}\")\n",
        "        print(\"-\" * (len(key) + 24)) # Divider\n",
        "\n",
        "    # --- Final Summary ---\n",
        "    print(\"\\n--- Final Summary ---\")\n",
        "    print(f\"Total Explores Scanned:          {total_explores}\")\n",
        "    print(f\"Successfully Analyzed & Parsed: {analysis_success}\")\n",
        "    print(f\"Fetch/Definition Errors:        {fetch_errors}\")\n",
        "    print(f\"Gemini Analysis Errors:         {analysis_errors}\")\n",
        "    print(f\"Successful Analysis, BUT Parsing Failed: {parsing_errors}\")\n",
        "    print(\"---------------------\")\n",
        "    total_processed_check = fetch_errors + analysis_errors + parsing_errors + analysis_success\n",
        "    print(f\"Total Processed:                {total_processed_check} (Should match Total Scanned: {total_explores})\")\n",
        "\n",
        "\n",
        "    # 8. Save Results to File\n",
        "    output_filename = \"lookml_conversational_analysis_results.json\"\n",
        "    print(f\"\\nStep 8: Saving detailed results to '{output_filename}'...\")\n",
        "    try:\n",
        "        # Optionally remove the raw explore definition before saving to keep JSON smaller\n",
        "        # Also remove raw analysis text if present\n",
        "        for key in parsed_results:\n",
        "             if \"explore_definition_json\" in parsed_results[key]:\n",
        "                 del parsed_results[key][\"explore_definition_json\"]\n",
        "             if \"raw_analysis\" in parsed_results[key]:\n",
        "                  del parsed_results[key][\"raw_analysis\"]\n",
        "\n",
        "\n",
        "        with open(output_filename, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(parsed_results, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Successfully saved results (excluding raw definitions/analysis).\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Failed to save results to JSON file '{output_filename}': {e}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_duration = end_time - start_time\n",
        "    print(\"\\n--- Script Finished ---\")\n",
        "    print(f\"Script finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total execution time: {total_duration:.2f} seconds\")\n",
        "\n",
        "# --- Script Entry Point ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure configuration values are set before running main\n",
        "    if \"your-gcp-project\" in looker_project_id or \\\n",
        "       \"your-looker-secret-name\" in looker_secret_name or \\\n",
        "       \"your-gcp-project\" in vertex_project_id:\n",
        "        # Removed check for history_data_filepath as it's no longer used\n",
        "        print(\"ERROR: Please update the placeholder values in the 'Configuration' section\")\n",
        "        print(\"       (looker_project_id, looker_secret_name, vertex_project_id)\")\n",
        "        print(\"       before running the script.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    main()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting LookML Conversational Readiness Analysis ---\n",
            "Script started at: 2025-04-18 18:20:25\n",
            "\n",
            "Step 1: Fetching Looker credentials...\n",
            "Attempting to fetch secret 'looker_ini' version 'latest' from project 'joey-looker'...\n",
            "Successfully fetched secret 'looker_ini'.\n",
            "Successfully parsed [Looker] section from secret.\n",
            "\n",
            "Step 2: Initializing Looker SDK...\n",
            "Initializing Looker SDK...\n",
            "Looker SDK initialized successfully. Connected as user: Super Admin (ID: 1)\n",
            "\n",
            "Step 3: Initializing Vertex AI...\n",
            "Initializing Vertex AI for project 'joey-looker' in location 'us-central1'...\n",
            "Vertex AI initialized successfully.\n",
            "Instantiating Gemini model: gemini-2.5-pro-exp-03-25...\n",
            "Gemini model instantiated successfully.\n",
            "\n",
            "Step 4: Fetching and Processing Looker History Data via API...\n",
            "Fetching and processing history data via Looker API...\n",
            "Running inline query on system__activity/history...\n",
            "Successfully fetched 57 history records via API.\n",
            "Finished processing history data:\n",
            "  - Total records processed from API: 57\n",
            "  - Records skipped (zero run/user count): 0\n",
            "  - Records skipped (field JSON parse error): 0\n",
            "  - Records skipped (no fields/not list/empty): 3\n",
            "\n",
            "Step 5: Determining explores to analyze...\n",
            "  Using the 1 specified target explore(s).\n",
            "Prepared 1 explores for analysis.\n",
            "\n",
            "Step 6: Starting analysis loop for 1 explores...\n",
            "\n",
            "--- Analyzing Explore 1/1: basic_ecomm/basic_order_items ---\n",
            "  Fetching definition for explore: basic_ecomm/basic_order_items...\n",
            "  Successfully fetched and formatted definition for basic_ecomm/basic_order_items.\n",
            "  Sending request to Gemini (gemini-2.5-pro-exp-03-25) for analysis of basic_ecomm/basic_order_items...\n",
            "  Received response from Gemini for basic_ecomm/basic_order_items.\n",
            "  Successfully extracted analysis text from Gemini response.\n",
            "  Analysis SUCCEEDED for basic_ecomm/basic_order_items.\n",
            "  Found 10 top used fields based on history.\n",
            "  Time taken for basic_ecomm/basic_order_items: 26.03 seconds\n",
            "\n",
            "--- Analysis Loop Complete ---\n",
            "\n",
            "Step 7: Processing and Outputting Results...\n",
            "\n",
            "--- Processing Results for basic_ecomm/basic_order_items ---\n",
            "  Parsing: SUCCESSFUL (Grade: 20, Recommendations: 8)\n",
            "  Generated 1 agent instruction(s).\n",
            "  Generated suggested CA LookML file content.\n",
            "  Top Used Fields Found: 10\n",
            "-----------------------------------------------------\n",
            "\n",
            "--- Final Summary ---\n",
            "Total Explores Scanned:          1\n",
            "Successfully Analyzed & Parsed: 1\n",
            "Fetch/Definition Errors:        0\n",
            "Gemini Analysis Errors:         0\n",
            "Successful Analysis, BUT Parsing Failed: 0\n",
            "---------------------\n",
            "Total Processed:                1 (Should match Total Scanned: 1)\n",
            "\n",
            "Step 8: Saving detailed results to 'lookml_conversational_analysis_results.json'...\n",
            "Successfully saved results (excluding raw definitions/analysis).\n",
            "\n",
            "--- Script Finished ---\n",
            "Script finished at: 2025-04-18 18:20:55\n",
            "Total execution time: 29.72 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# Make sure this matches the output filename from the main script\n",
        "input_json_filename = \"lookml_conversational_analysis_results.json\"\n",
        "# Base directory where the LookML files will be saved\n",
        "output_base_dir = \"ca_lookml_output\"\n",
        "# Suffix used in the generated LookML file content (should match main script)\n",
        "CA_SUFFIX = \"_ca\"\n",
        "\n",
        "# --- Helper function to sanitize names for filenames/paths ---\n",
        "def sanitize_name(name):\n",
        "    # Remove potentially problematic characters for filenames/directory names\n",
        "    # Keep alphanumeric and underscores\n",
        "    name = re.sub(r'[^\\w_]+', '_', name)\n",
        "    # Avoid names starting/ending with underscore or multiple underscores\n",
        "    name = re.sub(r'_+', '_', name).strip('_')\n",
        "    return name if name else \"invalid_name\"\n",
        "\n",
        "# --- Main Logic ---\n",
        "def save_lookml_files():\n",
        "    print(f\"Attempting to load results from: {input_json_filename}\")\n",
        "    try:\n",
        "        with open(input_json_filename, 'r', encoding='utf-8') as f:\n",
        "            results_data = json.load(f)\n",
        "        print(f\"Successfully loaded results for {len(results_data)} explores.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Input JSON file not found: '{input_json_filename}'\")\n",
        "        print(\"Please ensure the main script ran successfully and created the JSON file.\")\n",
        "        return\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"ERROR: Failed to decode JSON from '{input_json_filename}': {e}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: An unexpected error occurred while loading JSON: {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Saving LookML files to base directory: '{output_base_dir}'\")\n",
        "    saved_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    for explore_key, data in results_data.items():\n",
        "        lookml_content = data.get(\"suggested_ca_lookml_file\")\n",
        "\n",
        "        if not lookml_content or not isinstance(lookml_content, str):\n",
        "            print(f\"  Skipping {explore_key}: No LookML content found in results.\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            model_name, explore_name = explore_key.split('/', 1)\n",
        "\n",
        "            # Sanitize names for path creation\n",
        "            sane_model_name = sanitize_name(model_name)\n",
        "            sane_explore_name = sanitize_name(explore_name)\n",
        "\n",
        "            # Define output directory and filename\n",
        "            model_dir = os.path.join(output_base_dir, sane_model_name)\n",
        "            # Use the CA suffix convention for the filename\n",
        "            output_filename = f\"{sane_explore_name}{CA_SUFFIX}.explore.lkml\"\n",
        "            output_filepath = os.path.join(model_dir, output_filename)\n",
        "\n",
        "            # Create directory if it doesn't exist\n",
        "            os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "            # Write the LookML content to the file\n",
        "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(lookml_content)\n",
        "\n",
        "            print(f\"  Saved: {output_filepath}\")\n",
        "            saved_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to save LookML for '{explore_key}': {e}\")\n",
        "            skipped_count += 1\n",
        "\n",
        "    print(\"\\n--- File Saving Summary ---\")\n",
        "    print(f\"Successfully saved: {saved_count} file(s)\")\n",
        "    print(f\"Skipped/Errors:   {skipped_count}\")\n",
        "    print(f\"Output directory: '{os.path.abspath(output_base_dir)}'\")\n",
        "\n",
        "# --- Run the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    save_lookml_files()\n"
      ],
      "metadata": {
        "id": "LgJvp4aXClZW",
        "outputId": "57911cf9-2231-454e-c3e8-5ac897ab5ac4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LgJvp4aXClZW",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load results from: lookml_conversational_analysis_results.json\n",
            "Successfully loaded results for 1 explores.\n",
            "Saving LookML files to base directory: 'ca_lookml_output'\n",
            "  Saved: ca_lookml_output/basic_ecomm/basic_order_items_ca.explore.lkml\n",
            "\n",
            "--- File Saving Summary ---\n",
            "Successfully saved: 1 file(s)\n",
            "Skipped/Errors:   0\n",
            "Output directory: '/content/ca_lookml_output'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "admin (Apr 15, 2025, 10:03:16 AM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}